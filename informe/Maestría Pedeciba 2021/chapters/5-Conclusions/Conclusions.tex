
% Chapter ?
\chapter{Conclusions and Future Work} % Main chapter title
\label{conclusions} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 5. \emph{Conclusions and Future Work}} % This is for the header on each page - perhaps a shortened title


In this work, we study the compression of multichannel signals with irregular sampling rates and with data gaps. In Chapter~\ref{datasets} we present different real-word datasets consisting of signals with these characteristics, which are often gathered by WSNs. In this case, the irregular sampling rates are caused by different groups of sensors being out of sync or malfunctioning. Besides, errors might arise when acquiring, transmitting or storing the data, causing gaps in the data. Nevertheless, state-of-the-art algorithms designed for sensor data compression reported in the literature \cite{AnEva2013, Signal2016} assume, in general, that the signals gave regular sampling rate and that there are no data gaps. Thus, in this thesis we introduce a number of variants of state-of-the-art algorithms, which we design and implement so that they are able to encode multichannel signals with irregular sampling rates and data gaps. 


In Chapter~\ref{datasets}, besides presenting the experimental datasets, laying out the source, characteristics, and relevant statistics of every signal involved, we also describe a format that we define to represent their data. The datasets come from multiple sources \dataCite, each using a different data representation format. Thus, we transformed all of the data into an homogeneous format, which not only works for our experimental datasets, but can also be easily adapted to represent additional datasets in the future.


In Chapter~\ref{algo} we present our implemented variants for state-of-the-art algorithms. Both the original algorithms and our proposed variants follow a model-based compression approach: the signals are compressed by exploiting \textit{temporal correlation} (i.e. correlation among signal samples taken at close times), and, in some cases, \textit{spatial correlation} (i.e. correlation among samples from various signals). Model-based compression techniques offer an efficient compression performance, as well as some data processing features, such as inferring uncertain sensor readings, detecting outliers, indexing, etc. \cite{AnEva2013}. The model-based algorithms are classified into different categories, depending on the type of the model: \textit{constant model} algorithms, such as PCA \cite{coder:pca}, and APCA \cite{coder:apca}, approximate signals by piecewise constant functions; \textit{linear model} algorithms, such as PWLH \cite{coder:pwlh}, PWLHInt (see Subsection~\ref{algo:pwhl:int}), CA \cite{coder:ca}, and FR \cite{coder:fr}, approximate signals by linear functions; and \textit{correlation model} algorithms, such as GAMPS \cite{coder:gamps}, simultaneously encode multiple signals by exploiting temporal and spatial correlation.


Both state-of-the-art algorithms and our implemented variants are \textit{near-lossless} compression algorithms: they guarantee a bounded per-sample absolute error, which is specified via a parameter, denoted \maxerror. When \maxerror is zero, the compression is \textit{lossless}: the decompressed signal is identical to the original signal. For most algorithms we implement two variants, \textit{masking (\maskalgo)} and \textit{non-masking (\NOmaskalgo)}. In variant \maskalgo, the position of all the data gaps is encoded first, and the data values are encoded separately, next. In Section~\ref{algo:maskmodes} we describe our proposed strategy to compress the gaps in variant \maskalgo. It uses arithmetic coding \cite{ac2, Cover2005} combined with a Krichevsky-Trofimov probability assignment \cite{ktestimator} over a Markov model. On the other hand, in variant \NOmaskalgo, the gaps and the data values are encoded together. With both variants, the gaps in a decompressed signal always match the gaps in the original signal, regardless of the value of the error threshold parameter (\maxerror).


Our experimental results are presented in Chapter~\ref{experiments}. The main goal of our experiments is to analyze the compression performance of each of the algorithm variants presented in Chapter Chapter~\ref{algo} by encoding the various datasets introduced in Chapter~\ref{datasets}. For each variant, we test several combinations of parameters. In particular, we consider an extended range of values for the error threshold parameter, including the lossless case (i.e. $\maxerror=0$). We asses the compression performance of an algorithm variant through the \textit{compression ratio (CR)} metric (see Definition \ref{def:compression-rate}), and we compare the compression performance between a pair of algorithm variants through the \textit{relative difference (RD)} metric (see Definition~\ref{relative-difference}). 


In Section~\ref{secX:rendimiento-relativo}, for each algorithm $a$ that supports variants \maskalgo and \NOmaskalgo, we compare the respective compression performance of both, $a_\maskalgo$ and $a_\NOmaskalgo$. The results show that, on experimental datasets with few or no gaps, both variants have roughly the same performance, i.e. $\RD(a_\maskalgo, a_\NOmaskalgo)$ is always close to zero, ranging between $-0.29$ and $1.76\%$. However, on datasets with many gaps, variant \maskalgo always performs better, sometimes with a significant difference, with $\RD(a_\maskalgo, a_\NOmaskalgo)$ ranging between $2.44$ and $50.78\%$. Thus, our experimental results suggest that variant \maskalgo is more robust, and performs better in general.


State-of-the-art algorithms, as well as our implemented variants, depend on a window size parameter, $\win$, which defines the size of the blocks into which the data are partitioned for encoding. In both variants of algorithm PCA, parameter $w$ defines a \textit{fixed block size}, while for the rest of the algorithm variants it defines a \textit{maximum block size}. Our experiments consider an extended range of values for parameter $\win$, which allows us to analyze the sensitivity of the different algorithm variants to this parameter. For each dataset, we compress each data file, and compare the results obtained when using a window size optimized for said specific file, against the results obtained when using a window size optimized for the whole dataset. The results, which are presented in Section~\ref{secX:windows}, indicate that the difference in compression performance is generally rather small, with the RD being less than or equal to $2\%$ in $97.7\%$ of the experimental cases. Obtaining the optimal $\win$ for a specific data file is, in general, a computationally expensive task, so these results have meaningful practical consequences, since they imply that we could fix parameter $\win$ in advance, for example by optimizing over a training set, without compromising the overall performance of the compression algorithm variant. 


In Section~\ref{secX:codersmask} we present the most important part of our experimental analysis, which consists in comparing the compression performance of the algorithm variants, among each other, and with the general-purpose compression algorithm gzip \cite{gzip}. Following the experimental results presented in the previous two paragraphs, for this analysis we only consider variant \maskalgo of each algorithm, and we always use the optimal window size for the whole dataset. Our experimental results reveal that none of the algorithm variants obtains the best compression performance in every scenario, which means that the optimal selection of a variant depends on the characteristics of the data to be compressed, and the error threshold (\maxerror) that is allowed. In some cases, even a general-purpose compression algorithm such as gzip outperforms the specific variants. Nonetheless, we some general conclusions can be obtained from our analysis. For large error thresholds, variant \MaskVar{APCA} achieves the best results, obtaining the minimum CR in $90.5\%$ of the experimental cases. On the other hand, algorithm gzip and variant \MaskVar{PCA} are preferred for lower thresholds scenarios, since they obtain the minimum CR in $42.9\%$ and $40.5\%$ of the cases, respectively.


\clearpage


% We conclude this chapter with a few suggestions of ideas that could be developed as future work:
% We conclude this chapter suggesting a few ideas that could be developed as future work:
We conclude this chapter suggesting a few ideas to develop as future work:

\vspace{-5pt}

\begin{itemize}

\item In Section~\ref{algo:details} we mention that we focus our work on the compression of the sample columns of the datasets, and do not delve into the optimization of the timestamp compression. This is an interesting problem to investigate in the future.

\item In our experimental analysis, which is presented in Chapter~\ref{experiments}, we asses the compression performance of the implemented algorithm variants through the CR metric (recall Definition~\ref{def:compression-rate}). It would be interesting to consider additional metrics, such as the computational time and the sensitivity to outliers \cite{AnEva2013}. We could also consider additional datasets, which would increase the running time of our experiments, but may provide new insights regarding the compression performance of the implemented algorithms.

\item The results presented in Section~\ref{secX:codersmask} reveal \textit{which} of the implemented algorithm variants obtains better compression results for each data type in the experimental datasets. It would be useful to analyze \textit{why} is it the case that a certain variant achieves better results for a certain data type. In order to do this, we would need to examine the signals for each data type, analyze their characteristics (e.g. whether they are slowly varying or rough signals, the number of outliers, periodicity), and observe if there exists a relation between these characteristics and the algorithm variant that obtains the best compression performance. This would be useful to predict which variant is the best for compressing certain signal, only by analyzing the signal characteristics. If, given certain statistics of a signal, we could programmatically select a good compression algorithm variant for the signal, this could prove to be beneficial for online compression, as it would allow us to select a different variant as the trends in the signal vary over time.

\end{itemize}

