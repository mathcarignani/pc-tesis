
% Chapter ?
\chapter{Conclusions and Future Work} % Main chapter title
\label{conclusions} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 5. \emph{Conclusions and Future Work}} % This is for the header on each page - perhaps a shortened title


% \vspace{-10pt}
In this work, we study the compression of multichannel signals with irregular sampling rates and with data gaps. Our experiments consider different real-word datasets consisting of signals with these characteristics, which are often gathered by WSNs. In such case, the irregular sampling rates can be caused by different groups of sensors being out of sync or malfunctioning. Besides, errors might arise when acquiring, transmitting or storing the data, causing gaps in the data. Nevertheless, state-of-the-art algorithms designed for sensor data compression reported in the literature \cite{AnEva2013, Signal2016} assume, in general, that the signals gave regular sampling rate and that there are no data gaps. Thus, our main contributions are introducing and analyzing the compression performance of a number of variants of state-of-the-art algorithms, which we design and implement so that they are able to encode multichannel signals with irregular sampling rates and data gaps.


The experimental datasets come from multiple sources \dataCite, each using a different data representation format. Thus, we define our own format, into which we transformed all the experimental datasets. This format can be used to represent various kinds of datasets, and our implemented algorithm variants should be able to encode any data file that follows its specifications.


% State-of-the-art algorithms, as well as our implemented variants, are \textit{near-lossless} compression algorithms, in which the bounded per-sample absolute error is specified via parameter \maxerror.
% % (when \maxerror is zero, the compression is \textit{lossless}). 
% They follow a model-based compression approach: the signals are compressed by exploiting \textit{temporal correlation}, and, in some cases, \textit{spatial correlation}. The model-based algorithms are classified into different categories, depending on the type of the model: \textit{constant model} algorithms, such as PCA \cite{coder:pca}, and APCA \cite{coder:apca}, approximate signals by piecewise constant functions; \textit{linear model} algorithms, such as PWLH \cite{coder:pwlh}, PWLHInt (see Subsection~\ref{algo:pwhl:int}), CA \cite{coder:ca}, and FR \cite{coder:fr}, approximate signals by linear functions; and \textit{correlation model} algorithms, such as GAMPS \cite{coder:gamps}, simultaneously encode multiple signals by exploiting temporal and spatial correlation.


For most algorithms we implement two variants, \textit{masking (\maskalgo)} and \textit{non-masking (\NOmaskalgo)}. They differ in the technique used for encoding the position of the data gaps. In variant \maskalgo, the position of all the data gaps is encoded first, and the data values are encoded separately, next. Our proposed strategy to compress the gaps in variant \maskalgo uses arithmetic coding \cite{ac2, Cover2005} combined with a Krichevsky-Trofimov probability assignment \cite{ktestimator} over a Markov model. On the other hand, in variant \NOmaskalgo, the gaps and the data values are encoded together. With both variants, the gaps in a decompressed signal always match the gaps in the original signal, regardless of the value of the error threshold parameter (\maxerror).


The main objective of our experimental work is to analyze the compression performance of our implemented algorithm variants by encoding our various experimental datasets. For each variant, we test several combinations of parameters. In particular, we consider an extended range of values for the error threshold parameter, including the \textit{lossless compression} case (i.e. $\maxerror=0$). We asses the compression performance of an algorithm variant through the \textit{compression ratio (CR)} metric (see Definition \ref{def:compression-rate}), and we compare the compression performance between a pair of algorithm variants through the \textit{relative difference (RD)} metric (see Definition~\ref{relative-difference}). 


Our first analysis consists of comparing the compression performance of our implemented variants $a_\maskalgo$ and $a_\NOmaskalgo$, for each algorithm $a$ that supports both variants. The results show that, on experimental datasets with few or no gaps, both variants have roughly the same performance, i.e. $\RD(a_\maskalgo, a_\NOmaskalgo)$ is always close to zero, ranging between $-0.29$ and $1.76\%$. However, on datasets with many gaps, $a_\maskalgo$ always performs better, sometimes with a significant difference, with $\RD(a_\maskalgo, a_\NOmaskalgo)$ ranging between $2.44$ and $50.78\%$. Thus, our experimental results suggest that variant \maskalgo is more robust, and performs better in general.


Both state-of-the-art algorithms and our implemented variants depend on a window size parameter, $\win$. It defines a \textit{fixed window size} for both variants of algorithm PCA, and a \textit{maximum window size} for the rest of the variants. Our experiments consider an extended range of values for parameter $\win$, which allows us to analyze the sensitivity of the different algorithm variants to this parameter. For each dataset, we compress each data file, and compare the results obtained when using a window size optimized for said specific file, against the results obtained when using a window size optimized for the whole dataset. The results indicate that the difference in compression performance is generally rather small, with the RD being less than or equal to $2\%$ in $97.8\%$ of the experimental cases. Obtaining the optimal $\win$ for a specific data file is, in general, a computationally expensive task, so these results have meaningful practical consequences, since they imply that we could fix parameter $\win$ in advance, for example by optimizing over a training set, without compromising the overall compression performance of an algorithm variant. 


The most significant part of our experimental analysis consists of comparing the compression performance of our implemented algorithm variants, with each other, and with the general-purpose lossless compression algorithm gzip \cite{gzip}. Following the experimental results presented in the previous two paragraphs, for this analysis we only consider variant \maskalgo of each algorithm, and we always use the optimal window size for the whole dataset. Our experimental results reveal that none of the variants obtains the best compression performance in every scenario, which means that the optimal selection of a variant depends on the characteristics of the data to be compressed, and the error threshold (\maxerror) that is allowed. In some cases, even a general-purpose compression algorithm such as gzip outperforms the specific variants. Nonetheless, some general conclusions can be obtained from our analysis. For large error thresholds, variant \MaskVar{APCA} achieves the best results, obtaining the minimum CR in $90.5\%$ of the experimental cases. On the other hand, algorithm gzip and variant \MaskVar{PCA} are preferred for lower thresholds scenarios, since they obtain the minimum CR in $42.9\%$ and $40.5\%$ of the cases, respectively. We also analyze the lossless compression of the timestamp column, and the results indicate that variant \NonMaskVar{APCA} generally achieves the best results.


% \clearpage


We conclude this chapter suggesting a couple of ideas to develop as future work:

\vspace{-5pt}

\begin{itemize} 
%[leftmargin=*]

\item In our experimental analysis, which is presented in Chapter~\ref{experiments}, we asses the compression performance of the implemented algorithm variants through the CR metric (recall Definition~\ref{def:compression-rate}). It would be interesting to consider additional metrics, such as the computational time and the sensitivity to outliers \cite{AnEva2013}. We could also consider additional datasets, which would increase the running time of our experiments, but may provide new insights regarding the compression performance of the implemented algorithms.

\item The results presented in Section~\ref{secX:codersmask} reveal \textit{which} of the implemented algorithm variants obtains better compression results for each data type in the experimental datasets. \Revisor{This information should be taken into account as part of a strategy to implement a universal coder (i.e. a coder that compresses signals from which no information is known a priori). For this purpose, it would be useful to analyze \textit{why} it is the case that a certain variant achieves better compression results for a certain data type over other variants.} In order to do this, we would need to examine the signals for each data type, analyze their characteristics (e.g. whether they are smooth or rough signals, the number of outliers, periodicity), and observe if there exists a relation between these characteristics and the algorithm variant that obtains the best compression performance. This would be useful for predicting which variant is the best for compressing certain signal, only by analyzing the signal characteristics. If, given certain statistics of a signal, we could programmatically select a good compression algorithm variant for the signal, this could prove to be beneficial for online compression, \Revisor{as it would allow a universal coder to select a different variant as the trends in the signal vary over time.}
\end{itemize}


\clearpage


\chapter*{Acknowledgements}
% \thispagestyle{empty}

I would like to express my sincere gratitude to my supervisors, Álvaro Martín and Gadiel Seroussi. I thank \textit{Programa de Desarrollo de las Ciencias Básicas (PEDECIBA)} and \textit{Agencia Nacional de Investigación e Innovación (ANII)} for their financial support.

\clearpage