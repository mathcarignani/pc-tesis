

\section{Introduction}
\label{algo:overview}


The state-of-the-art algorithms used for sensor data compression reported in the literature \cite{AnEva2013, Signal2016} assume, in general, that the signals have regular sampling rate and that there are no gaps in the data. However, this is sometimes not true for real-world datasets. For example, all the datasets presented in Chapter~\ref{datasets} consist of signals that miss either one or both of these characteristics. We propose a number of variants of state-of-the-art algorithms, which are able to encode this type of data. 


We focus on algorithms that support near-lossless compression. Near-lossless compression guarantees a bounded per-sample error between the decompressed and the original signals. The error threshold can be specified by the user via a parameter, denoted~$\maxerror$. When $\maxerror$ is equal to 0, the compression is lossless, i.e., the decompressed and the original signals are identical.


The algorithms follow a model-based compression approach that compresses signals by exploiting correlation among signal samples taken at close times (\textit{temporal correlation}) and, in some cases, among samples from various signals (\textit{spatial correlation}). In addition to efficient compression performance, they offer some data processing features, like inferring uncertain sensor readings, detecting outliers, indexing, etc. \cite{AnEva2013}. The model-based techniques are classified into different categories, depending on the type of model: \textit{constant models} approximate signals by piecewise constant functions, \textit{linear models} use linear functions, and \textit{correlation models} simultaneously encode multiple signals exploiting temporal and spatial correlation. There also exist \textit{nonlinear models}, which approximate signals by nonlinear functions, but known algorithms that follow this technique do not support near-lossless compression and yield poor compression results~\cite{AnEva2013}. 


For most algorithms we propose two variants, \textit{masking (\maskalgo)} and \textit{non-masking (\NOmaskalgo)}, which differ in the encoding of the gaps in the data. Variant \maskalgo of an algorithm first encodes the position of all the gaps, and then proceeds to encode the data values separately. On the other hand, variant \NOmaskalgo encodes the gaps and the data values together. Implementation details are presented in the remaining sections of this chapter. We point out that the gaps in a decoded file match the gaps in the original file exactly, regardless of the value of the error threshold parameter (\maxerror). In Section~\ref{secX:rendimiento-relativo}, we compare the compression performance of both variants, \maskalgo\ and \NOmaskalgo, for every evaluated algorithm that supports both.


Except from algorithm Base, all of the algorithms depend on a window size parameter, denoted $w$, which defines the size of the blocks into which the data are divided for encoding. In one of the algorithms, PCA (to be presented in Section~\ref{algo:pca}), parameter $w$ defines a \textit{fixed block size}, while in the rest of the algorithms it defines a \textit{maximum block size}.
% Algorithm GAMPS, to be presented in Section~\ref{algo:gamps}, is an \textit{offline algorithm}: it is given all of the data to be compressed as one of its inputs. On the other hand, the rest of the algorithms are \textit{online algorithms}, since they receive the data to be compressed sequentially. 
More details are presented with the specific description of each algorithm, later in this chapter.


In Table~\ref{algo:table:overview} we outline some characteristics of the evaluated algorithms and the proposed variants. For each algorithm, the second and third columns indicate whether it supports lossless and near-lossless compression, respectively, the fourth column shows its compression model, the fifth and sixth columns indicate if the masking (\maskalgo) and non-masking (\NOmaskalgo) variants apply, respectively, and the last column specifies if the algorithm depends on a window size parameter ($w$). Algorithm Base is a trivial lossless algorithm that is used as a base ground for comparing the performance of the remaining algorithms, all of which support both lossless and near-lossless encoding.


\clearpage


\input{chapters/3-Algorithms/other/algo-table}

