Automatically generated by Mendeley Desktop 1.13.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop


%%% CITAR
% COVER
% INTRODUCTION TO DATA COMPRESSION
% LOSSLESS COMPRESSION HANDBOOK

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LIBROS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{Cover2005,
abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
archivePrefix = {arXiv},
arxivId = {ISBN 0-471-06259-6},
author = {Cover, Thomas M. and Thomas, Joy A.},
booktitle = {Elements of Information Theory},
doi = {10.1002/047174882X},
eprint = {ISBN 0-471-06259-6},
isbn = {9780471241959},
issn = {0162-1459},
pages = {1--748},
pmid = {20660925},
title = {{Elements of Information Theory}},
year = {2006},
edition = {2nd},
publisher = {Wiley-Interscience}
}

@book{Sayood2012,
author = {Sayood, Khalid},
booktitle = {Introduction to Data Compression},
title = {{Introduction to Data Compression}},
year = {2012},
edition = {4th},
publisher = {Morgan Kaufmann}
}

@book{Sayood2002,
author = {Sayood, Khalid},
booktitle = {Lossless Compression Handbook},
title = {{Lossless Compression Handbook}},
year = {2002},
publisher = {Academic Press}
}

@book{mp3,
author = {Hacker, Scot},
booktitle = {MP3: The Definitive Guide},
title = {{MP3: The Definitive Guide}},
year = {2000},
edition = {1st},
publisher = {O'Reilly Media}
}

@book{tvdigital,
author = {Robin, Michael and Poulin, Michel},
booktitle = {Digital Television Fundamentals},
title = {{Digital Television Fundamentals}},
year = {2000},
edition = {2nd},
publisher = {McGraw-Hill Education}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% INTERNET - CIENCIA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{marte1,
  author = {Fabiola Czubaj},
  title = {{Un equipo rioplatense creó el sistema que comprime las fotos}},
  howpublished = "\url{http://www.lanacion.com.ar/563203-un-equipo-rioplatense-creo-el-sistema-que-comprime-las-fotos}",
  year = {12-01-2004}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{marte2,
  author = {Jamie Beckett},
  title = {{HP on Mars: Labs technology used to send most accurate images possible}},
  howpublished = "\url{http://www.hpl.hp.com/news/2004/jan-mar/hp_mars.html}",
  year = {Enero 2004}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{pluton1,
  author = {Mia Tramz},
  title = {{Photographing Pluto: This Is How New Horizons Works}},
  howpublished = "\url{http://www.time.com/3944157/new-horizons-pluto/}",
  year = {14-07-2015}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{pluton2,
  author = {The Johns Hopkins University Applied Physics Laboratory LLC},
  title = {{Science Operations Center}},
  howpublished = "\url{http://www.pluto.jhuapl.edu/Pluto/Science-Operations-Center.php}",
  year = {2015}, 
  note = "[Dirección web. Accedida el \mydate]"
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% INTERNET - COMERCIAL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{digitalUniverse,
  author = {Turner, Vernon and Reinsel, David and Gantz, John F. and Minton, Stephen},
  title = {{The Digital Universe of Opportunities: Rich Data and the Increasing Value of the Internet of Things}},
  howpublished = "\url{http://idcdocserv.com/1678}",
  year = {Abril 2014}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{skype,
  author = {Mo Ladha},
  title = {{Skype’s Pursuit of the Perfect Video Call}},
  howpublished = "\url{http://blogs.skype.com/2014/01/06/skypes-pursuit-of-the-perfect-video-call/}",
  year = {6-01-2014}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{spotify,
  author = {Ty Pendlebury},
  title = {{Spotify vs. Apple Music: Is there a difference in sound quality?}},
  howpublished = "\url{http://www.cnet.com/news/apple-music-vs-spotify-is-there-a-difference-in-sound-quality/}",
  year = {5-07-2014}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{youtube,
  author = {Geoffrey Morrison},
  title = {{What is HEVC? High Efficiency Video Coding, H.265, and 4K compression explained}},
  howpublished = "\url{http://www.cnet.com/news/what-is-hevc-high-efficiency-video-coding-h-265-and-4k-compression-explained/}",
  year = {18-04-2014}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{nikon,
  title = {{DSLR Camera Basics: Image Quality and Size}},
  howpublished = "\url{http://imaging.nikon.com/lineup/dslr/basics/26/01.htm}",
  year = {2014}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{canon,
  title = {{Image compression: Lossless and lossy compression}},
  howpublished = "\url{http://cpn.canon-europe.com/content/education/infobank/image_compression/lossless_and_lossy_compression.do}",
  year = {2015}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{gzip,
  title = {{GNU Gzip}},
  howpublished = "\url{https://www.gnu.org/software/gzip/}",
  year = {2010}, 
  note = "[Dirección web. Accedida el \mydate]"
}

@misc{winrar,
  title = {{WinRAR at a glance}},
  howpublished = "\url{http://www.win-rar.com/features.html?&L=0}",
  year = {2015}, 
  note = "[Dirección web. Accedida el \mydate]"
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% FORMATOS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Shorten1994,
title = {{SHORTEN: Simple lossless and near-lossless waveform compression}},
author = {Robinson, Tony},
year = {Dec. 1994},
journal = {Technical Report 156, Engineering Department, Cambridge University}
}

@misc{flac,
  author = {Josh Coalson},
  title = {{FLAC format}},
  howpublished = "\url{https://xiph.org/flac/format.html}",
  year = {2014}, 
  note = "[Dirección web. Accedida el \mydate]"
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% PAPERS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{Pras2009,
abstract = {Mp3 compression is commonly used to reduce the size of digital music files but introduces a number of potentially audible artifacts, especially at low bitrates. We investigated whether listeners prefer CD quality to mp3 files at various bitrates (96 kb/s to 320 kb/s), and whether this preference is affected by musical genre. Thirteen trained listeners completed an A/B comparison task judging CD quality and compressed files. Listeners significantly preferred CD quality to mp3 files up to 192 kb/s for all musical genres. In addition, we observed a significant effect of expertise (sound engineers vs. musicians) and musical genres (electric v.s acoustic music).},
author = {Pras, Amandine and Zimmerman, Rachel and Letvin, Daniel and Guastavino, Catherine},
booktitle = {Audio Engineering Society, 127th Convention 2009},
isbn = {9781615677122},
pages = {1--7},
title = {{Subjective evaluation of MP3 compression for different musical genres}},
year = {2009}
}

@article{McMillan1956,
abstract = {Consider a list of<tex>b</tex>words, each word being a string of letters from a given fixed alphabet of<tex>a</tex>letters. If every string of words drawn from this list, when written out in letters without additional space marks to separate the words, is uniquely decipherable, then begin\{equation\} a\^{}\{-l\_1\} + a\^{}\{-l\_2\} + cdots + a\^{}\{-l\_b\} leq 1, qquad qquad (1) end\{equation\} where<tex>l\_i, 1 leq i leq b</tex>, is the length of the<tex>i</tex>th word in the list. This result extends a remark of J. L. Doob, who derived the same inequality for lists of a more restricted kind. A consequence of (1) and work of Shannon is that this more restricted kind of list suffices in the search for codes with specified amounts of redundancy.},
author = {McMillan, Brockway},
doi = {10.1109/TIT.1956.1056818},
issn = {0096-1000},
journal = {IRE Transactions on Information Theory},
number = {4},
title = {{Two inequalities implied by unique decipherability}},
volume = {2},
year = {1956}
}

@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E.},
doi = {10.1145/584091.584093},
eprint = {9411012},
isbn = {0252725484},
issn = {07246811},
journal = {The Bell System Technical Journal},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
volume = {27},
year = {1948}
}

@article{Gallager1976,
author = {Gallager, Robert G.},
title = {{Source coding with side information and universal coding}},
journal = {Proc. IEEE Int. Symp. Information Theory},
year = {1976}
}


@article{Huffman1952,
abstract = {An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.},
author = {Huffman, David A.},
doi = {10.1109/JRPROC.1952.273898},
isbn = {1111022020101},
issn = {0096-8390},
journal = {Proceedings of the IRE},
number = {9},
title = {{A Method for the Construction of Minimum-Redundancy Codes}},
volume = {40},
year = {1952}
}

@article{Golomb1966,
abstract = {Run-length encodings, determining explicit form of Huffman coding when applied to geometric distribution},
author = {Golomb, Solomon W.},
doi = {10.1109/TIT.1966.1053907},
isbn = {1111111111},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {3},
title = {{Run-length encodings (Corresp.)}},
volume = {12},
year = {1966}
}

@article{Slepian1973,
abstract = { Correlated information sequences<tex>cdots ,X\_\{-1\},X\_0,X\_1, cdots</tex>and<tex>cdots,Y\_\{-1\},Y\_0,Y\_1, cdots</tex>are generated by repeated independent drawings of a pair of discrete random variables<tex>X, Y</tex>from a given bivariate distribution<tex>P\_\{XY\} (x,y)</tex>. We determine the minimum number of bits per character<tex>R\_X</tex>and<tex>R\_Y</tex>needed to encode these sequences so that they can be faithfully reproduced under a variety of assumptions regarding the encoders and decoders. The results, some of which are not at all obvious, are presented as an admissible rate region<tex>mathcal\{R\}</tex>in the<tex>R\_X - R\_Y</tex>plane. They generalize a similar and well-known result for a single information sequence, namely<tex>R\_X geq H (X)</tex>for faithful reproduction.},
author = {Slepian, David and Wolf, Jack K.},
doi = {10.1109/TIT.1973.1055037},
isbn = {0018-9448 VO - 19},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {4},
title = {{Noiseless coding of correlated information sources}},
volume = {19},
year = {1973}
}

@article{Gallager1975,
abstract = { Let<tex>P(i)= (1 - theta)theta\^{}i</tex>be a probability assignment on the set of nonnegative integers where<tex>theta</tex>is an arbitrary real number,<tex>0 \&amp;lt; theta \&amp;lt; 1</tex>. We show that an optimal binary source code for this probability assignment is constructed as follows. Let<tex>l</tex>be the integer satisfying<tex>theta\^{}l + theta\^{}\{l+1\} leq 1 \&amp;lt; theta\^{}l + theta\^{}\{l-1\}</tex>and represent each nonnegative integer<tex>i</tex>as<tex>i = lj + r</tex>when<tex>j = lfloor i/l rfloor</tex>, the integer part of<tex>i/l</tex>, and<tex>r = [i] mod l</tex>. Encode<tex>j</tex>by a unary code (i.e.,<tex>j</tex>zeros followed by a single one), and encode<tex>r</tex>by a Huffman code, using codewords of length<tex>lfloor log\_2 l rfloor</tex>, for<tex>r \&amp;lt; 2\^{}\{lfloor log l+1 rfloor\} - l</tex>, and length<tex>lfloor log\_2 l rfloor + 1</tex>otherwise. An optimal code for the nonnegative integers is the concatenation of those two codes.},
author = {Gallager, Robert G. and van Voorhis, David C.},
doi = {10.1109/TIT.1975.1055357},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {2},
title = {{Optimal source codes for geometrically distributed integer alphabets (Corresp.)}},
volume = {21},
year = {1975}
}

@article{Wyner1976,
abstract = { Let<tex>\{(X\_\{k\}, Y\_\{k\}) \}\^{}\{ infty\}\_\{k=1\}</tex>be a sequence of independent drawings of a pair of dependent random variables<tex>X, Y</tex>. Let us say that<tex>X</tex>takes values in the finite set<tex>cal X</tex>. It is desired to encode the sequence<tex>\{X\_\{k\}\}</tex>in blocks of length n into a binary stream of rate<tex>R</tex>, which can in turn be decoded as a sequence<tex>\{ hat\{X\}\_\{k\} \}</tex>, where<tex>hat\{X\}\_\{k\} in hat\{ cal X\}</tex>, the reproduction alphabet. The average distortion level is<tex>(1/n) sum\^{}\{n\}\_\{k=1\} E[D(X\_\{k\},hat\{X\}\_\{k\})]</tex>, where<tex>D(x,hat\{x\}) geq 0, x in \{cal X\}, hat\{x\} in hat\{ cal X\}</tex>, is a preassigned distortion measure. The special assumption made here is that the decoder has access to the side information<tex>\{Y\_\{k\}\}</tex>. In this paper we determine the quantity<tex>R ast (d)</tex>, defined as the infimum ofrates<tex>R</tex>such that (with<tex>varepsilon \&amp;gt; 0</tex>arbitrarily small and with suitably large<tex>n</tex>)communication is possible in the above setting at an average distortion level (as defined above) not exceeding<tex>d + varepsilon</tex>. The main result is that<tex>R ast (d) = inf [I(X;Z) - I(Y;Z)]</tex>, where the infimum is with respect to all auxiliary random variables<tex>Z</tex>(which take values in a finite set<tex>cal Z</tex>) that satisfy: i)<tex>Y,Z</tex>conditionally independent given<tex>X</tex>; ii) there exists a function<tex>f: \{cal Y\} times \{cal Z\} rightarrow hat\{ cal X\}</tex>, such that<tex>E[D(X,f(Y,Z))] leq d</tex>. Let<tex>R\_\{X | Y\}(d)</tex>be the rate-distortion function which results when the encoder as well as the decoder has access to the side information<tex>\{ Y\_\{k\} \}</tex>. In nearly all cases it is shown that when<tex>d \&amp;gt; 0</tex>then<tex>R ast(d) \&amp;gt; R\_\{X|Y\} (d)</tex>, so that knowledge of the side information at the encoder permits transmission of the<tex>\{X\_\{k\}\}</tex>at a given distortion level using a smaller transmission rate. This is in contrast to the situation treated by Slepian and Wolf [5] where, for arbitrarily accurate reproduction of<tex>\{X\_\{k\}\}</tex>, i.e.,<tex>d = varepsilon</tex>for any<tex>varepsilon \&amp;gt;0</tex>, knowledge of the side information at the-
 encoder does not allow a reduction of the transmission rate.},
author = {Wyner, Abraham J. and Ziv, Jacob},
doi = {10.1109/TIT.1976.1055508},
isbn = {0018-9448 VO - 22},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {1},
title = {{The rate-distortion function for source coding with side information at the decoder}},
volume = {22},
year = {1976}
}

@article{Hans2001,
abstract = {Lossless audio compression is likely to play an important part in music distribution over the Internet, DVD audio, digital audio archiving, and mixing. The article is a survey and a classification of the current state-of-the-art lossless audio compression algorithms. This study finds that lossless audio coders have reached a limit in what can be achieved for lossless compression of audio. It also describes a new lossless audio coder called AudioPak, which low algorithmic complexity and performs well or even better than most of the lossless audio coders that have been described in the literature},
author = {Hans, M. and Schafer, R. W.},
doi = {10.1109/79.939834},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {4},
pages = {21--32},
title = {{Lossless compression of digital audio}},
volume = {18},
year = {2001}
}

@article{Bassino2013,
abstract = {Lossless compression is studied for pairs of independent, integer-valued symbols emitted by a source with a geometric probability distribution of parameter q, 0 \&amp;lt; q \&amp;lt; 1. Optimal prefix codes are described for q = 1/2<sup>k</sup> (k \&amp;gt; 1) and q = 1/knthroot2 (k \&amp;gt; 0). These codes retain some of the low-complexity and low-latency advantage of symbol by symbol coding of geometric distributions, which is widely used in practice, while improving on the inherent redundancy of the approach. From a combinatorial standpoint, the codes described differ from previously characterized cases related to the geometric distribution in that their corresponding trees are of unbounded width, and in that an infinite set of distinct optimal codes is required to cover any interval (0,epsi), epsi \&amp;gt; 0, of values of q},
archivePrefix = {arXiv},
arxivId = {1102.2413},
author = {Bassino, Fr\'{e}d\'{e}rique and Cl\'{e}ment, Julien and Seroussi, Gadiel and Viola, Alfredo},
doi = {10.1109/TIT.2012.2236915},
eprint = {1102.2413},
isbn = {1424405041},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Codes for countable alphabets,Geometric distributions,Golomb codes,Huffman codes,Lossless compression,Prefix codes},
number = {4},
pages = {2375--2395},
title = {{Optimal prefix codes for pairs of geometrically distributed random variables}},
volume = {59},
year = {2013}
}

@article{Kiely2004,
abstract = {We examine the use of Golomb-power-of-2 (GPO2) codes to efficiently and loss- lessly encode sequences of nonnegative integers from a discrete source. Specifically, we’re interested in the problem of selecting which GPO2 code to use for a source or a block of samples; this problem is at the heart of the well-known Rice entropy cod- ing algorithm. We’re particularly concerned with the case where the mean sample value is known or can be estimated. We derive bounds on the optimum code parameter as a function of the mean sample value. These bounds establish that no more than three possible code choices can be optimum for a given mean sample value. We derive a simple method to select the optimum GPO2 code for a geometrically distributed source given the mean value. We also devise a simpler code selection procedure that generalizes previously known methods. Both code selection methods can be implemented using only integer arithmetic and table look-ups, and require no divisions. We show that, for any source with known mean, the GPO2 code parameter selected under this simple procedure is always within one of the optimum code parameter for the source, and that the added cost due to suboptimum parameter selection under this procedure is never more than 1/2 bit per sample and no more than about 13 percent inefficiency. We investigate the use of both code selection procedures as lower complexity alternatives to Rice coding within the emerging Consultative Committee for Space Data Systems (CCSDS) image compression standard. For the images tested, both code selection methods produce negligible added rate compared to optimal code selection as in Rice coding.},
author = {Kiely, Aaron B.},
journal = {IPN Progress Report, vol. 42-159, pp. 1–8},
title = {{Selecting the Golomb Parameter in Rice Coding}},
volume = {2},
year = {November 15, 2004}
}

@article{Hans2001,
abstract = {Lossless audio compression is likely to play an important part in music distribution over the Internet, DVD audio, digital audio archiving, and mixing. The article is a survey and a classification of the current state-of-the-art lossless audio compression algorithms. This study finds that lossless audio coders have reached a limit in what can be achieved for lossless compression of audio. It also describes a new lossless audio coder called AudioPak, which low algorithmic complexity and performs well or even better than most of the lossless audio coders that have been described in the literature},
author = {Hans, Mat and Schafer, Ronald W.},
doi = {10.1109/79.939834},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {4},
pages = {21--32},
title = {{Lossless compression of digital audio}},
volume = {18},
year = {2001}
}

@article{Weinberger2000,
abstract = {LOCO-I (LOw COmplexity LOssless COmpression for Images) is the algorithm at the core of the new ISO/ITU standard for lossless and near-lossless compression of continuous-tone images, JPEG-LS. It is conceived as a "low complexity projection" of the universal context modeling paradigm, matching its modeling unit to a simple coding unit. By combining simplicity with the compression potential of context models, the algorithm "enjoys the best of both worlds." It is based on a simple fixed context model, which approaches the capability of the more complex universal techniques for capturing high-order dependencies. The model is tuned for efficient performance in conjunction with an extended family of Golomb-type codes, which are adaptively chosen, and an embedded alphabet extension for coding of low-entropy image regions. LOCO-I attains compression ratios similar or superior to those obtained with state-of-the-art schemes based on arithmetic coding. Moreover, it is within a few percentage points of the best available compression ratios, at a much lower complexity level. We discuss the principles underlying the design of LOCO-I, and its standardization into JPEC-LS.},
author = {Weinberger, Marcelo J. and Seroussi, Gadiel and Sapiro, Guillermo},
doi = {10.1109/83.855427},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Context modeling,Geometric distribution,Golomb codes,Lossless image compression,Near-lossless compression,Standards},
number = {8},
pages = {1309--1324},
pmid = {18262969},
title = {{The LOCO-I lossless image compression algorithm: Principles and standardization into JPEG-LS}},
volume = {9},
year = {2000}
}

@article{Rice1979,
title = {{Some Practical Universal Noiseless Coding Techniques—Parts I-III}},
author = {Rice, Robert F.},
year = {Mar. 1979, Mar. 1983, Nov. 1991},
journal = {Jet Propulson Lab., Pasadena, CA, Tech. Reps. JPL-79-22, JPL-83-17, and JPL-91-3}
}

@inproceedings{casson2008wearable,
  title={Wearable {EEG}: what is it, why is it needed and what does it entail?},
  author={Casson, Alexander J. and Smith, Shelagh and Duncan, John S. and Rodriguez-Villegas, Esther},
  booktitle={Engineering in medicine and biology society, 2008. embs 2008. 30th annual international conference of the ieee},
  pages={5867--5870},
  year={2008},
  organization={IEEE}
}
