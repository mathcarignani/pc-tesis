

\section{Introduction}
\label{algo:overview}


The state-of-the-art algorithms used for sensor data compression reported in the literature \cite{AnEva2013, Signal2016} assume, in general, that the signals have regular sampling and that there are no gaps in the data. However, this is often not true for real-world datasets. For example, all the datasets presented in Chapter~\ref{datasets} consist of signals that miss either one or both of these characteristics. We propose a number of variants of state-of-the-art algorithms, which are able to encode this type of signals. 


We focus on algorithms that support near-lossless compression. Near-lossless compression guarantees a bounded per-sample error between the decompressed and the original signals. The error threshold can be specified by the user via a parameter, denoted~$\maxerror$. Observe that, when $\maxerror$ is equal to 0, compression is lossless, i.e., the decompressed and the original signals are identical.


The algorithms follow a model-based compression approach that compresses signals by exploiting correlation among signal samples taken at close times (\textit{temporal correlation}) and, in some cases, among samples from various signals (\textit{spatial correlation}). In addition to efficient compression performance, they offer some data processing features, like inferring uncertain sensor readings, detecting outliers, indexing, etc. \cite{AnEva2013}. The model-based techniques are classified into different categories, depending on the type of model: \textit{Constant models} approximate signals by piecewise constant functions, \textit{Linear models} use linear functions, and \textit{Correlation models} simultaneously encode multiple signals exploiting temporal and spatial correlation. There also exist \textit{Nonlinear models}, which approximate signals by complex nonlinear functions, but known algorithms that follow this technique do not support near-lossless compression and yield poor compression results~\cite{AnEva2013}. 


For most algorithms we propose two variants, masking (\maskalgo) and non-masking (\NOmaskalgo), which differ in the encoding of the gaps in the data. The \maskalgo\ variant of an algorithm first encodes the position of all the gaps, and then proceeds to encode the data values separately. On the other hand, the \NOmaskalgo\ variant encodes the gaps and the data values simultaneously. Implementation details are presented in the remaining sections of this chapter. We point out that the gaps in a decoded file match the gaps in the original file exactly, regardless of the value of the error threshold parameter (\maxerror). In Section~\ref{secX:rendimiento-relativo} we compare the compression performance of both variants, \maskalgo\ and \NOmaskalgo, for every algorithm that supports both.


Most of the algorithms support a window size parameter, denoted $w$, which defines the size of the blocks into which the data are divided for encoding. In algorithm PCA, parameter $w$ defines a \textit{fixed block size}, while in the rest of the algorithms it defines a \textit{maximum block size}. More details are presented with the specific description of each algorithm, later in this chapter.


In Table~\ref{algo:table:overview} we outline some characteristics of the evaluated algorithms and the proposed variants. For each algorithm, the second and third columns indicate whether it supports lossless and near-lossless compression, respectively, the fourth column shows its model type, the fifth and sixth columns indicate if the masking (\maskalgo) and non-masking (\NOmaskalgo) variants apply, respectively, and the last column specifies if the algorithm depends on a window size parameter ($w$). Algorithm Base is a trivial lossless algorithm that is used as a base ground for comparing the performance of the remaining algorithms, all of which support both lossless and near-lossless encoding.


\clearpage


\input{chapters/Algorithms/other/algo-table}

